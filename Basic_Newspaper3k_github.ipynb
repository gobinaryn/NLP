{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sentiment Analysis of News Article Using Newspaper for Python 3.x, Gensim, and NLTK"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is a newspaper library which runs within Python 2.x. Newspaper library will not run on Python 3.x. Please use !pip3 install newspaper3k for functionality.\n",
    "\n",
    "Newspaper3k is a Python library used for Web Scraping news articles. The library was built on top of the requests library and allows for access to news articles from popular news sites. Newspaper3k allows for users to access the information without performing sometimes laborious and tedious webscraping that often comes with limitations on what you can download (and how many times), authorization tokens, and paywalls rquired to access a website's Application Programming Interface (API). Newspaper also provides access to text information immediately that can be cleaned and that is already somewhat structured. Regular Expressions would help fill in the gaps for extracting text information where newspaper3k fails to do so.\n",
    "\n",
    "The documents for the library can be found here: https://newspaper.readthedocs.io/en/latest/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip3 install newspaper3k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import newspaper\n",
    "from newspaper import Article\n",
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "article = Article('https://www.reuters.com/technology/apple-supplier-foxconn-apologises-hiring-blunder-covid-hit-china-plant-2022-11-24/')\n",
    "article.download()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When selecting a news article, please remember that articles that are on the \"front page\" of a news online website may change location. Your code might not work. Suggestion is to use an article with a more stable URL if possible"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To print the full text of the article, use the .text -- no parentheses -- method on the variable assigned to the article's URL\n",
    "To print the keywords, use .keywords method on the variable assigned to the article's URL\n",
    "Additional basic exploratory analysis of a Natural Language Processing flavor follows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "article.parse()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(article.text)\n",
    "print()\n",
    "print('-----------------------------End of Article, Summary Below---------------')\n",
    "print()\n",
    "print(article.summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Authors: \" + str(article.authors))\n",
    "print(\"Publish Date: \" + str(article.publish_date))\n",
    "print(\"Keywords: \" + str(article.keywords))\n",
    "print('\\n')\n",
    "print(\"This is the URL for the article's first image: \" + article.top_image)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With this particular news article, photos are photos embedded in the article that are denoted by text similar to \"© Reuters\" or could reflect the news agency who published the report. We can remove this information from the text as it is \"noise\" or unnecessary text. \n",
    "\n",
    "Noise in Natural Language Processing, or when working with any kind of text data, can present itself the form of special characters such as hashtags, punctuation and numbers. Noise can also refer to spam or anything that impacts the integrity of your data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using Regex to parse out image description data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#This is EXTRA code in case your article has photographs with captions\n",
    "#import re #imports the regular expressions library for Python\n",
    "#test_string = article.text\n",
    "#pattern = 'REUTERS.+'\n",
    "\n",
    "#result = re.findall(pattern, test_string)\n",
    "#print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To remove the copyright information captions associated with the images, we are going to use the simple .replace() method using the string we want to replace and defining what we _want to replace it with._ If there were more, a for loop could be used or code to filter out the unwanted text. This is optional code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "news_text = str(article.text)\n",
    "#news2 = news_text.replace(\"© Reuters/MAXIM SHEMETOV Russian Navy Day celebrations in Saint Petersburg\", \" \")\n",
    "#.replace(\"© Reuters/RITZAU SCANPIX Copenhagen mall shooting\" ,\" \")\n",
    "#print(news2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the first steps of this process, we called the newspaper3k .keywords method on article. A few of the keywords are \"scanpix\" and \"reutersritzau\" -- the same text that appeared in the text that is not part of the article -- remember these are the photo captions and the photo captions do not contribute anything to the substantive content of the article. We have to re-call the keywords again. This illustrates the importance of using domain knowledge: reviewing the data, journalism and formatting of news articles and recognizing that python libraries need some human assistance every once in a while.\n",
    "\n",
    "The data type of our first variable \"article\" is a class and the .replace method does not work on class. We converted the class to a string and will have to employ another program to generate the keywords. \n",
    "\n",
    "__Programs that can extract keywords:__\n",
    "- Gensim\n",
    "- SpaCy\n",
    "- YAKE (Yet Another Keyword Extractor)\n",
    "- Rapid Automatic Keyword Extraction (RAKE) algorithm with the NLTK\n",
    "\n",
    "\n",
    "For the purpose of this exercise, we are going to use the library Gensim as we want to conduct the simple identification/generation of keywords. SpaCy and Gensim are probably the most accessible with the lowest learning curve. To use this version of Gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "from gensim.summarization import keywords\n",
    "print(keywords(news_text))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quick Exercises Using Natural Language Toolkit (NLTK)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stopwords and Removing Extra punctuation "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before running tokenization methods on the text, we need to further clean the text to eliminate special characteres such as question marks, parentheses (opening and closing), commas (,), semicolons (;), apostrophes ('), single quotes (''), and double quotes (\"\")."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#This is EXTRA CODE\n",
    "#import re\n",
    "#news2_word_clean_punct = news_text.replace('\"', '').replace(\"\\'s\", '').replace(\"'\",'').replace('.', '').replace(';', '').replace(',', '').replace('-', '').replace('(', '').replace(')','').replace('\"\"', '')\n",
    "#news2_word_clean_punct\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "stop_words_nltk = set(stopwords.words('english'))\n",
    "\n",
    "stopwords_corpus = word_tokenize(str(news2_word_clean_punct))\n",
    "print(\"\\nNLTK\\nTokenized corpus:\",stopwords_corpus)\n",
    "tokenized_corpus_without_stopwords = [i for i in stopwords_corpus if not i in stop_words_nltk]\n",
    "print('\\n')\n",
    "print(\"Tokenized corpus without stopwords:\",tokenized_corpus_without_stopwords)\n",
    "print('\\n')\n",
    "print(\"Tokenized corpus with stop words\", len(stopwords_corpus))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A word cloud is popular visualization tool and technique that allows for the display of a corpus. The bigger the font size, the higher the more times a word appears in the corpus (or text). Wordclouds can be customized to fit a predefined mask (will determine the shape of your wordcloud), to display a color palette, and font type."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install wordcloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from wordcloud import WordCloud\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "wordcloud = WordCloud(width=3000, height=2000, background_color='turquoise', \n",
    "                       colormap='Pastel2', collocations=False).generate(str(news_text))\n",
    "\n",
    "         \n",
    "plt.figure(figsize=[15,10])                        # set the figsize\n",
    "plt.imshow(wordcloud, interpolation=\"bilinear\")    # plot the wordcloud\n",
    "plt.axis(\"off\")                                    # remove plot axes\n",
    "plt.savefig('wordcloud.png')                       # save as png\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
